{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25697cf-579c-4d22-9832-39b10e888003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b0eb15-b54c-4374-8a02-8a38dae42526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from neural_networks.nn import NeuralNetwork\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda32182-1785-4a34-a1a1-2a573801efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        > Takes input for initialized neural network and implements optimization of parameter weights based on the algorithm\n",
    "        \n",
    "        > Results: <br>\n",
    "        return: None\n",
    "        '''\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        return \"\"\"This class contains various optimizer and helper functions in one \n",
    "                    place for better and modular understanding of overall library.\"\"\"\n",
    "    @staticmethod\n",
    "    def gradientDescentOptimizer(\n",
    "        input,\n",
    "        mappings,\n",
    "        net,\n",
    "        alpha=0.001,\n",
    "        lamb=0, \n",
    "        epoch=100,\n",
    "        print_at=5,\n",
    "        prnt=True,\n",
    "        update=True):\n",
    "        '''\n",
    "        Performs gradient descent on the given network setting the \n",
    "            default value of epoch and alpha if not provided otherwise\n",
    "\n",
    "        Parameters:\n",
    "        input: input for neural net\n",
    "        mapping: Correct output of the function\n",
    "        net: nn.nn object which provides the network architecture\n",
    "        alpha: Learning rate\n",
    "        lamb: Regularization parameter\n",
    "        epoch: Number of iterations\n",
    "        print_at: Print at multiples of 'print_at'\n",
    "        prnt: Print if prnt=true #alias for verbose mode\n",
    "        \n",
    "        Results:\n",
    "        return: None\n",
    "        '''\n",
    "        net.lamb = lamb\n",
    "        algo = \"gradient-descent\"\n",
    "        if prnt:\n",
    "            print(\"==\"*10, f\"Optimizer: {algo}\", \"==\"*10)\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            net.cache = []\n",
    "            prediction = net.forward(input)\n",
    "            loss_function = (net.cost_function).lower()\n",
    "            loss,regularization_cost = None,0\n",
    "            if loss_function == 'mseloss':\n",
    "                loss = net.MSELoss(prediction,mappings)\n",
    "            elif loss_function == 'crossentropyloss':\n",
    "                loss = net.CrossEntropyLoss(prediction,mappings)\n",
    "            else:\n",
    "                raise Exception(\"invalid loss function\")\n",
    "            \n",
    "            if prnt and i%print_at==0 :\n",
    "                print(\"----->> Loss at \",i, \": \" ,np.round(loss,5))\n",
    "\n",
    "            net.backward(prediction,mappings)\n",
    "            if update:\n",
    "                net.parameters = Optimizer.update_params(net.parameters,net.grads,alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def SGDOptimizer(input,\n",
    "        mappings,\n",
    "        net,\n",
    "        mini_batch_size=64,\n",
    "        alpha=0.001,\n",
    "        lamb=0,\n",
    "        momentum=None,\n",
    "        epoch=5,\n",
    "        print_at=5,\n",
    "        prnt=True):\n",
    "        '''\n",
    "        Performs Stochaitic gradient descent on the given network\n",
    "            -Generates mini batches of given size using random permutation\n",
    "            -Uses gradient descent on each mini batch separately\n",
    "\n",
    "        Parameters:\n",
    "        input: input for neural net\n",
    "        mapping: Correct output of the function\n",
    "        net: nn.nn object which provides the network architecture\n",
    "        batch_size: Batch size to be used witch SGD\n",
    "        alpha: Learning rate\n",
    "        lamb: Regularization parameter\n",
    "        momentum: Momentum Hyper parameter\n",
    "        epoch: Number of iterations\n",
    "        print_at: Print at multiples of 'print_at'\n",
    "        prnt: Print if prnt=true\n",
    "\n",
    "        Results:\n",
    "        return: None\n",
    "        '''\n",
    "        batch_size = input.shape[1]\n",
    "        mini_batches = []\n",
    "        # if momentium\n",
    "        if momentum:\n",
    "            algo = \"stochastic-gradient-descent-with-momentum\"\n",
    "        else:\n",
    "            algo = \"stochastic-gradient-descent\"\n",
    "        if prnt:\n",
    "            print(\"==\"*10, f\"Optimizer: {algo}\", \"==\"*10)\n",
    "        \n",
    "        permutation = list(np.random.permutation(batch_size))\n",
    "        shuffled_input = input[:,permutation]\n",
    "        shuffled_mappings = (mappings[:,permutation])\n",
    "\n",
    "        num_complete_batches = int(np.floor(batch_size/mini_batch_size))\n",
    "        \n",
    "        #Separate the complete mini_batches\n",
    "        for i in range(0,num_complete_batches):\n",
    "            mini_batch_input = shuffled_input[:,i*mini_batch_size:(i+1)*mini_batch_size]\n",
    "            mini_batch_mappings = shuffled_mappings[:,i*mini_batch_size:(i+1)*mini_batch_size]\n",
    "            mini_batch = (mini_batch_input,mini_batch_mappings)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        #Separate the incomplete mini batch if any\n",
    "        if batch_size % mini_batch_size != 0:\n",
    "            mini_batch_input = shuffled_input[:,batch_size - num_complete_batches*mini_batch_size : batch_size]\n",
    "            mini_batch_mappings = shuffled_mappings[:,batch_size - num_complete_batches*mini_batch_size : batch_size]\n",
    "            mini_batch = (mini_batch_input,mini_batch_mappings)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        #Initialize momentum velocity\n",
    "        velocity = {}\n",
    "        if momentum != None:\n",
    "            for i in range(int(len(net.parameters)/2)):\n",
    "                velocity['dW'+str(i+1)] = np.zeros(net.parameters['W'+str(i+1)].shape)\n",
    "                velocity['db'+str(i+1)] = np.zeros(net.parameters['b'+str(i+1)].shape)\n",
    "        \n",
    "\n",
    "        for i in range(1,epoch+1):\n",
    "            for batches in range(len(mini_batches)):\n",
    "\n",
    "                if momentum != None:\n",
    "                    Optimizer.gradientDescentOptimizer(input,mappings,net,alpha,lamb,epoch=1,prnt=False,update=False)\n",
    "                    for j in range(int(len(net.parameters)/2)):\n",
    "                        velocity['dW' + str(j+1)] = momentum*velocity['dW'+str(j+1)] + (1-momentum)*net.grads['dW'+str(j+1)]\n",
    "                        velocity['db' + str(j+1)] = momentum*velocity['db'+str(j+1)] + (1-momentum)*net.grads['db'+str(j+1)]\n",
    "                    net.parameters = Optimizer.update_params(net.parameters,velocity,alpha)\n",
    "                else:\n",
    "                    Optimizer.gradientDescentOptimizer(input,mappings,net,alpha,lamb,epoch=1,prnt=False)\n",
    "\n",
    "            prediction = net.forward(input)\n",
    "            loss = None \n",
    "            loss_function = net.cost_function.lower()\n",
    "            if loss_function == 'mseloss':\n",
    "                loss = net.MSELoss(prediction,mappings)\n",
    "            if loss_function == 'crossentropyloss':\n",
    "                loss = net.CrossEntropyLoss(prediction,mappings)\n",
    "            \n",
    "            if i%print_at == 0:\n",
    "                print(\"----->> Loss at \",i, \": \" ,np.round(loss,5))\n",
    "    \n",
    "    @staticmethod\n",
    "    def AdamOptimizer(input,\n",
    "        mappings,\n",
    "        net,\n",
    "        alpha=0.001,\n",
    "        lamb=0,\n",
    "        betas=(0.9,0.99),\n",
    "        epoch=5,\n",
    "        print_at=5,\n",
    "        prnt=True):\n",
    "        '''\n",
    "        Performs Adam otimization on the given network.\n",
    "\n",
    "        Parameters:\n",
    "        input: input for neural net\n",
    "        mapping: Correct output of the function\n",
    "        net: nn.nn object which provides the network architecture\n",
    "        alpha: Learning rate\n",
    "        lamb: Regularization parameter\n",
    "        betas: Adam Hyper parameters\n",
    "        epoch: Number of iterations\n",
    "        print_at: Print at multiples of 'print_at'\n",
    "        prnt: Print if prnt=true\n",
    "\n",
    "        Results:\n",
    "        return: None\n",
    "        '''\n",
    "        batch_size = input.shape[1]\n",
    "        algo = \"adaptive-momentum-estimation(ADAM)\"\n",
    "        if prnt:\n",
    "            print(\"==\"*10, f\"Optimizer: {algo}\", \"==\"*10)\n",
    "        \n",
    "        velocity, square = {},{}\n",
    "        for i in range(int(len(net.parameters)/2)):\n",
    "            velocity['dW'+str(i+1)] = np.zeros(net.parameters['W'+str(i+1)].shape)\n",
    "            velocity['db'+str(i+1)] = np.zeros(net.parameters['b'+str(i+1)].shape)\n",
    "            square['dW'+str(i+1)] = np.zeros(net.parameters['W'+str(i+1)].shape)\n",
    "            square['db'+str(i+1)] = np.zeros(net.parameters['b'+str(i+1)].shape)\n",
    "        \n",
    "        for i in range(1,epoch+1):\n",
    "            \n",
    "            Optimizer.gradientDescentOptimizer(input,mappings,net,lamb,epoch=1,prnt=False,update=False)\n",
    "\n",
    "            for j in range(int(len(net.parameters)/2)):\n",
    "                velocity['dW'+str(j+1)] = betas[0]*velocity['dW'+str(j+1)] + (1-betas[0])*net.grads['dW'+str(j+1)]\n",
    "                velocity['db'+str(j+1)] = betas[0]*velocity['db'+str(j+1)] + (1-betas[0])*net.grads['db'+str(j+1)]\n",
    "                square['dW'+str(j+1)] = betas[1]*square['dW'+str(j+1)] + (1-betas[1])*np.power(net.grads['dW'+str(j+1)],2)\n",
    "                square['db'+str(j+1)] = betas[1]*square['db'+str(j+1)] + (1-betas[1])*np.power(net.grads['db'+str(j+1)],2)\n",
    "            \n",
    "            update = {}\n",
    "            for j in range(int(len(net.parameters)/2)):\n",
    "                update['dW' + str(j+1)] = velocity['dW'+ str(j+1)]/(np.sqrt(square['dW'+str(j+1)])+1e-10)\n",
    "                update['db' + str(j+1)] = velocity['db'+ str(j+1)]/(np.sqrt(square['db'+str(j+1)])+1e-10)\n",
    "            \n",
    "            net.parameters = Optimizer.update_params(net.parameters,update,alpha)\n",
    "\n",
    "            prediction = net.forward(input)\n",
    "            loss = None \n",
    "            loss_function = net.cost_function.lower()\n",
    "            if loss_function == 'mseloss':\n",
    "                loss = net.MSELoss(prediction,mappings)\n",
    "            if loss_function == 'crossentropyloss':\n",
    "                loss = net.CrossEntropyLoss(prediction,mappings)\n",
    "            \n",
    "            if i%print_at == 0:\n",
    "                print(\"----->> Loss at \",i, \": \" ,np.round(loss,5))\n",
    "\n",
    "    @staticmethod\n",
    "    def update_params(params,updation,learning_rate):\n",
    "        '''\n",
    "        Updates the parameters using gradients and learning rate provided\n",
    "        \n",
    "        Parameters:\n",
    "        params: Parameters of the network\n",
    "        updation: updation valcues calculated using appropriate algorithms\n",
    "        learning_rate: Learning rate for the updation of values in params\n",
    "\n",
    "        Results\n",
    "        return: Updated params \n",
    "        '''\n",
    "        \n",
    "        for i in range(int(len(params)/2)):\n",
    "            params['W' + str(i+1)] = params['W' + str(i+1)] - learning_rate*updation['dW' + str(i+1)]\n",
    "            params['b' + str(i+1)] = params['b' + str(i+1)] - learning_rate*updation['db' + str(i+1)]\n",
    "        \n",
    "        return params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
