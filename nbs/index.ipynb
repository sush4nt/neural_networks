{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from neural_networks.nn import NeuralNetwork\n",
    "from neural_networks.optimizer import Optimizer\n",
    "import sklearn.datasets as datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural_networks\n",
    "\n",
    "> Creating a python implementation of neural networks.\n",
    "\n",
    "> It contains two classes: <br>\n",
    "    1. `NeuralNetwork`: takes input and performs forward + backward propogation based on the loss functions defined. <br>\n",
    "    2. `Optimizers`: contains Batch-Gradient-Descent and ADAM optimization methods to update network weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install neural_networks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. This is a module which provides a `NeuralNetwork` class to build a neural network.\n",
    "\n",
    "Inputs:\n",
    "1. input layer dimensions\n",
    "2. activation functions for hidden layers + output layers. some of the valid activation functions are:\n",
    "    1. \"relu\"\n",
    "    2. \"sigmoid\"\n",
    "    3. \"tanh\"\n",
    "    4. \"softmax\"\n",
    "3. loss functions:\n",
    "    1. \"MSE\"\n",
    "    2. \"CrossEntropyLoss\"\n",
    "\n",
    "#### This module also contains a `Optimizer` class which takes in the training data and optimizes the parameter weights based on the type of optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a binary classification test\n"
     ]
    }
   ],
   "source": [
    "# test run for binary classification problem:\n",
    "np.random.seed(3)\n",
    "print('Running a binary classification test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (10, 30000)\n",
      "Output shape:  (1, 30000)\n"
     ]
    }
   ],
   "source": [
    "data = datasets.make_classification(n_samples=30000,n_features=10,n_classes=2)\n",
    "X = data[0].T\n",
    "Y = (data[1].reshape(30000,1)).T\n",
    "\n",
    "print(\"Input shape: \", X.shape)\n",
    "print(\"Output shape: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Network Architecture ====================\n",
      "----->> Layer 1: ( 10,20 ) |  activation: relu | No of parameters: 200\n",
      "----->> Layer 2: ( 20,1 ) |  activation: sigmoid | No of parameters: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate sample binary classification data\n",
    "net = NeuralNetwork(layer_dimensions=[10,20,1],\n",
    "                    activations=['relu','sigmoid'])\n",
    "net.cost_function = 'CrossEntropyLoss'\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Optimizer: gradient-descent ====================\n",
      "----->> Loss at  0 :  0.7462\n",
      "----->> Loss at  100 :  0.24641\n",
      "for gradient descent \n",
      " accuracy =  91.65\n"
     ]
    }
   ],
   "source": [
    "#Optimize using standard gradient descenet\n",
    "optim = Optimizer.gradientDescentOptimizer\n",
    "optim(input=X,\n",
    "      mappings=Y,\n",
    "      net=net,\n",
    "      alpha=0.07,\n",
    "      epoch=200,\n",
    "      lamb=0.05,\n",
    "      print_at=100)\n",
    "output = net.forward(X)\n",
    "\n",
    "#Convert the probabilities to output values\n",
    "output = 1*(output>=0.5)\n",
    "accuracy = np.sum(output==Y)/30000\n",
    "print('for gradient descent \\n accuracy = ' ,np.round(accuracy*100,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
