{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from numpy import float64, ndarray\n",
    "from typing import List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, \n",
    "        layer_dimensions: list = [], \n",
    "        activations: list = []) -> None:\n",
    "        '''\n",
    "        > Initializes networks's weights and other useful variables.\n",
    "        \n",
    "        > Parameters: <br>\n",
    "        `layer_dimensions`: specify the dimensions for each hidden + output layer\n",
    "        `activations`: to store the activation for each hidden + output layer\n",
    "        \n",
    "        > Attributes: <br>\n",
    "        -`parameters` contains weights of the layer in form {'Wi':[],'bi':[]} <br>\n",
    "        -`cache` contains intermediate results as [[A[i-1],Wi,bi],[Zi]], where i\n",
    "             is layer number.\n",
    "        -`activations` contains the names of activation function used for that layer <br>\n",
    "        -`cost_function`  contains the name of cost function to be used <br>\n",
    "        -`lamb` contains the regularization hyper-parameter <br>\n",
    "        -`grads` contains the gradients calculated during back-prop in form {'dA(i-1)':[],'dWi':[],'dbi':[]} <br>\n",
    "        -`layer_type` contains the info about the type of layer( fc, conv etc) <br>\n",
    "        \n",
    "        > Results: <br>\n",
    "        return: None\n",
    "        '''\n",
    "        self.parameters = {}\n",
    "        self.cache = []\n",
    "        self.activations = activations\n",
    "        self.cost_function = ''\n",
    "        self.lamb = 0\n",
    "        self.grads = {}\n",
    "        self.layer_type = ['']\n",
    "        self.hyperparam = {}\n",
    "        self.initialize_parameters(layer_dimensions)\n",
    "        self.check_activations()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"\"\"Class containing functionality to build and train neural networks. \\n\n",
    "                  Contains all activation and loss functions some other utility function.\"\"\"\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        '''\n",
    "        Returns: the network architecture and connectivity\n",
    "        '''\n",
    "        print(\"==\"*10, \"Network Architecture\", \"==\"*10)\n",
    "        net_string = \"\"\n",
    "        for params in range(int(len(self.parameters)/2)):\n",
    "            weight = self.parameters['W'+str(params+1)]\n",
    "            if self.activations[params] != None:\n",
    "                print(f\"----->> Layer {params+1}: ( {str(weight.shape[1])},\"\n",
    "                      f\"{str(weight.shape[0])} ) |  activation: {self.activations[params]} \"\n",
    "                      f\"| No of parameters: {weight.shape[1]*weight.shape[0]}\") \n",
    "        return net_string\n",
    "\n",
    "    def initialize_parameters(self, \n",
    "        layer_dimensions: list) -> None:\n",
    "        '''\n",
    "        `Xavier initialization` of weights of a network described by given layer\n",
    "        dimensions.\n",
    "        \n",
    "        Parameters:\n",
    "        layer_dimensions: dimensions to layers of the network\n",
    "        \n",
    "        Results:\n",
    "        return: None\n",
    "        '''\n",
    "        num_layers = int(len(self.parameters)/2)\n",
    "\n",
    "        for i in range(1, len(layer_dimensions)):\n",
    "            sq_root = np.sqrt( 2 / (layer_dimensions[i - 1]+layer_dimensions[i]) )\n",
    "            m_shape = np.random.randn(layer_dimensions[i], layer_dimensions[i - 1])\n",
    "            \n",
    "            self.parameters[\"W\" + str(num_layers+i)] = ( sq_root * m_shape )\n",
    "            self.parameters[\"b\" + str(i+num_layers)] = np.zeros((layer_dimensions[i], 1))\n",
    "            self.layer_type.append('fc')\n",
    "\n",
    "    def add_fcn(self,\n",
    "        dims: list,\n",
    "        activations: list) -> None:\n",
    "        '''\n",
    "        Add fully connected layers in between the network\n",
    "        \n",
    "        Parameters:\n",
    "        dims: list describing dimensions of fully connected networks\n",
    "        activations: activations of each layer\n",
    "        '''\n",
    "        self.initialize_parameters(dims)\n",
    "        for i in activations:\n",
    "            self.activations.append(i)\n",
    "\n",
    "    def check_activations(self) -> None:\n",
    "        '''\n",
    "        Checks if activations for all layers are present. Adds 'None' if no activations are provided for a particular layer.\n",
    "        \n",
    "        Results:\n",
    "        returns: None\n",
    "        '''\n",
    "        num_layers = int(len(self.parameters)/2)\n",
    "        while len(self.activations) < num_layers :\n",
    "            self.activations.append(None)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def __linear_forward(A_prev: np.array, \n",
    "        W: np.array, \n",
    "        b: np.array):\n",
    "        '''\n",
    "        Linear forward to the current layer using previous activations.\n",
    "        \n",
    "        Parameters:\n",
    "        param A_prev: previous Layer's activation\n",
    "        param W: weights for current layer\n",
    "        param b: biases for current layer\n",
    "        \n",
    "        Results:\n",
    "        Z: current calculated layer\n",
    "        linear_cache: linear cache ( previous act i/p, weights ) \n",
    "        '''\n",
    "        Z = W.dot(A_prev) + b\n",
    "        linear_cache = [A_prev, W, b]\n",
    "        return Z, linear_cache\n",
    "\n",
    "    def __activate(self, \n",
    "        Z, \n",
    "        n_layer=1):\n",
    "        \"\"\"\n",
    "        Activate the given layer(Z) using the activation function specified by\n",
    "            'type'.\n",
    "\n",
    "        Note: This function treats 1 as starting index!\n",
    "              First layer's index is 1.\n",
    "\n",
    "        Parameters:\n",
    "        param Z: layer to activate\n",
    "        param n_layer: layer's index\n",
    "        \n",
    "        Results:\n",
    "        act: activated layer\n",
    "        act_cache: activation cache\n",
    "        \"\"\"\n",
    "        act_cache = [Z]\n",
    "        act = None\n",
    "        if (self.activations[n_layer - 1]) == None:\n",
    "            act = Z\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"relu\":\n",
    "            act = Z * (Z > 0)\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"tanh\":\n",
    "            act = np.tanh(Z)\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"sigmoid\":\n",
    "            act = 1 / (1 + np.exp(-Z))\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"softmax\":\n",
    "            act = np.exp(Z-np.max(Z))\n",
    "            act = act/(act.sum(axis=0)+1e-10)\n",
    "        return act, act_cache\n",
    "\n",
    "    def forward(self, \n",
    "        net_input: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        To forward propagate the entire neural network.\n",
    "\n",
    "        ParametersL\n",
    "        net_input: contains the input to the network\n",
    "        :return: output of the network\n",
    "        \"\"\"\n",
    "        self.cache = [] \n",
    "        A = net_input\n",
    "        for i in range(1, int(len(self.parameters) / 2)):\n",
    "            W = self.parameters[\"W\" + str(i)]\n",
    "            b = self.parameters[\"b\" + str(i)]\n",
    "            Z = linear_cache = None\n",
    "            if self.layer_type[i] == 'fc':\n",
    "                Z, linear_cache = self.__linear_forward(A, W, b)\n",
    "            elif self.layer_type[i] == 'conv':\n",
    "                hyperparam = self.hyperparam[i]\n",
    "                Z , linear_cache = self.conv_forward(A,W,b,hyperparam)\n",
    "\n",
    "                #flatten the output if the next layer is fully connected\n",
    "            A, act_cache = self.__activate(Z, i)\n",
    "            if  self.layer_type[i]=='conv':\n",
    "                if  self.layer_type[i+1] == 'fc':\n",
    "                    A = A.reshape((A.shape[1]*A.shape[2]*A.shape[3],A.shape[0]))\n",
    " \n",
    "\n",
    "            self.cache.append([linear_cache, act_cache])\n",
    "\n",
    "        # For Last Layer\n",
    "        W = self.parameters[\"W\" + str(int(len(self.parameters) / 2))]\n",
    "        b = self.parameters[\"b\" + str(int(len(self.parameters) / 2))]\n",
    "        Z, linear_cache = self.__linear_forward(A, W, b)\n",
    "        if len(self.activations) == len(self.parameters) / 2:\n",
    "            A, act_cache = self.__activate(Z, len(self.activations))\n",
    "            self.cache.append([linear_cache, act_cache])\n",
    "        else:\n",
    "            A = Z\n",
    "            self.cache.append([linear_cache, [None]])\n",
    "        return A\n",
    "    '''\n",
    "    !!!!Only works for fully connected networks.!!!!!\n",
    "\n",
    "    def forward_upto(self, net_input, layer_num):\n",
    "        \"\"\"\n",
    "        Calculates forward prop upto layer_num.\n",
    "\n",
    "        :param net_input: Contains the input to the Network\n",
    "        :param layer_num: Layer up to which forward prop is to be calculated\n",
    "        :return: Activations of layer layer_num\n",
    "        \"\"\"\n",
    "        if layer_num == int(len(self.parameters) / 2):\n",
    "            return self.forward(net_input)\n",
    "        else:\n",
    "            A = net_input\n",
    "            for i in range(1, layer_num):\n",
    "                W = self.parameters[\"W\" + str(i)]\n",
    "                b = self.parameters[\"b\" + str(i)]\n",
    "                Z, linear_cache = self.__linear_forward(A, W, b)\n",
    "\n",
    "                A, act_cache = self.__activate(Z, i)\n",
    "                self.cache.append([linear_cache, act_cache])\n",
    "            return A\n",
    "    ''' \n",
    "\n",
    "    def MSELoss(self, \n",
    "        prediction: np.array,\n",
    "        mappings: np.array) -> float:\n",
    "        '''\n",
    "        Calculates the `Mean Squared Error` with regularization cost(if provided) between \n",
    "            output of the network and the real mappings of a function.\n",
    "        Changes `cost_function` to appropriate value\n",
    "\n",
    "        Parameters:\n",
    "        prediction: output of the neural net\n",
    "        mappings: real outputs of a function\n",
    "        \n",
    "        Results:\n",
    "        return: mean squared error b/w output and mappings\n",
    "        '''\n",
    "        self.cost_function = 'MSELoss'\n",
    "        loss = np.square(prediction-mappings).mean()/2\n",
    "        regularization_cost = 0\n",
    "        if self.lamb != 0:\n",
    "            for params in range(len(self.cache)):  \n",
    "                regularization_cost = regularization_cost + np.sum( np.square( self.parameters['W'+str(params+1)] ) ) #L2 regn\n",
    "        regularization_cost = (self.lamb/(2*prediction.shape[1]))*regularization_cost\n",
    "        return loss + regularization_cost\n",
    "\n",
    "    def CrossEntropyLoss(self,\n",
    "        prediction: np.array,\n",
    "        mappings: np.array) -> float:\n",
    "        '''\n",
    "        Calculates the cross entropy loss between output of the network and the real mappings of a function\n",
    "        Changes `cost_function` to appropriate value\n",
    "        \n",
    "        Parameters:\n",
    "        prediction: output of the neural net\n",
    "        mappings: real outputs of a function\n",
    "        \n",
    "        Results:\n",
    "        return: mean squared error b/w output and mappings\n",
    "        '''\n",
    "        epsilon = 1e-8\n",
    "        self.cost_function = 'CrossEntropyLoss'\n",
    "        ce_loss = np.sum( mappings*np.log(prediction+epsilon) + (1-mappings)*np.log(1-prediction+epsilon) )\n",
    "        loss = -(1/prediction.shape[1])*ce_loss\n",
    "        regularization_cost = 0\n",
    "        if self.lamb != 0:\n",
    "            for params in range(len(self.cache)):\n",
    "                regularization_cost = regularization_cost + np.sum(np.square(self.parameters['W'+str(params+1)]))\n",
    "        regularization_cost = (self.lamb/(2*prediction.shape[1]))*regularization_cost\n",
    "        return loss + regularization_cost\n",
    "    \n",
    "    def output_backward(self,\n",
    "        prediction: np.array,\n",
    "        mapping: np.array) -> np.array:\n",
    "        '''\n",
    "        Calculates the derivative of the output layer(dA)\n",
    "        \n",
    "        Parameters:\n",
    "        prediction: output of neural network\n",
    "        mapping: correct output of the function\n",
    "        cost_type: type of cost function used\n",
    "        \n",
    "        Results:\n",
    "        return: derivative of output layer, dA  \n",
    "        '''\n",
    "        dA = None\n",
    "        cost = self.cost_function\n",
    "        if cost.lower() == 'crossentropyloss':\n",
    "            # -1 * [ y/a + 1-y/1-a ]\n",
    "            dA =  -(np.divide(mapping, prediction+1e-10) - np.divide(1 - mapping, 1 - prediction+1e-10))\n",
    "        elif cost.lower() == 'mseloss':   \n",
    "            dA =  (prediction-mapping)\n",
    "        return dA\n",
    "    \n",
    "    def __deactivate(self,\n",
    "        dA: np.array,\n",
    "        n_layer: int) -> Union[np.array, int]:\n",
    "        '''\n",
    "        Calculates the derivate of dA by deactivating the layer\n",
    "\n",
    "        Parameters:\n",
    "        dA: Activated derivative of the layer\n",
    "        n_layer: Layer number to be deactivated\n",
    "        \n",
    "        Results\n",
    "        return: deact=> derivative of activation \n",
    "        '''\n",
    "        act_cache = self.cache[n_layer-1][1]\n",
    "        dZ = act_cache[0]\n",
    "        deact = None\n",
    "        if self.activations[n_layer - 1] == None:\n",
    "            deact = 1\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"relu\":\n",
    "            deact = 1* (dZ>0)\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"tanh\":\n",
    "            deact = 1- np.square(dA)\n",
    "        elif (self.activations[n_layer - 1]).lower() == \"sigmoid\" or (self.activations[n_layer - 1]).lower()=='softmax':\n",
    "            s = 1/(1+np.exp(-dZ+1e-10))\n",
    "            deact = s*(1-s)\n",
    "        return deact\n",
    "    \n",
    "    def __linear_backward(self,\n",
    "        dA: np.array,\n",
    "        n_layer: int) -> Tuple[np.array, np.array, np.array]:\n",
    "        '''\n",
    "        Calculates linear backward propragation for layer denoted by n_layer\n",
    "\n",
    "        Parameters:\n",
    "        dA: Derivative of cost w.r.t this layer\n",
    "        n_layer: layer number\n",
    "        \n",
    "        Results:\n",
    "        return : dZ,dW,db,dA_prev\n",
    "        '''\n",
    "        batch_size = dA.shape[1]\n",
    "        current_cache = self.cache[n_layer-1]\n",
    "        linear_cache = current_cache[0]\n",
    "        A_prev,W,b = linear_cache\n",
    "\n",
    "        dZ = dA*self.__deactivate(dA,n_layer)\n",
    "        dW = (1/batch_size)*dZ.dot(A_prev.T) + (self.lamb/batch_size)*self.parameters['W'+str(n_layer)]\n",
    "        db = (1/batch_size)*np.sum(dZ,keepdims=True,axis=1)\n",
    "        dA_prev = W.T.dot(dZ)\n",
    "\n",
    "        assert dA_prev.shape == A_prev.shape, \"derivatives' shape dont match: A_prev\"\n",
    "        assert dW.shape == W.shape, \"derivatives' shape dont match: dW\"\n",
    "        assert db.shape == b.shape, \"derivatives' shape dont match: db\"\n",
    "        \n",
    "        return dW,db,dA_prev\n",
    "\n",
    "    def backward(self,\n",
    "        prediction: np.array,\n",
    "        mappings: np.array) -> None:\n",
    "        '''\n",
    "        Backward propagates through the network and stores useful calculations\n",
    "        \n",
    "        Parameters:\n",
    "        prediction: Output of neural net\n",
    "        mapping: Correct output of the function\n",
    "        \n",
    "        Results:\n",
    "        return : None\n",
    "        '''\n",
    "        layer_num = len(self.cache)\n",
    "        doutput = self.output_backward(prediction, mappings)\n",
    "        self.grads['dW'+str(layer_num)], self.grads['db'+str(layer_num)], self.grads['dA'+str(layer_num-1)] = self.__linear_backward(doutput,layer_num)\n",
    "        temp = self.layer_type\n",
    "        self.layer_type = self.layer_type[1:]\n",
    "        \n",
    "        for l in reversed(range(layer_num-1)):\n",
    "            dW,db,dA_prev = None,None,None\n",
    "            if self.layer_type[l] == 'fc':\n",
    "                dW,db,dA_prev = self.__linear_backward(self.grads['dA'+str(l+1)],l+1)\n",
    "            elif self.layer_type[l] == 'conv':\n",
    "                dW,db,dA_prev = self.conv_backward((self.cache[l][1][0]),self.cache[l][0])\n",
    "            self.grads['dW'+str(l+1)] = dW\n",
    "            self.grads['db'+str(l+1)] = db\n",
    "            self.grads['dA'+str(l)] = dA_prev\n",
    "        \n",
    "        self.layer_type = temp\n",
    "    \n",
    "    @staticmethod\n",
    "    def zero_pad(\n",
    "        imgData: np.array,\n",
    "        pad: int) -> np.array:\n",
    "        '''\n",
    "        Provides zero padding to the multi channel image data provided\n",
    "        \n",
    "        Parameters:\n",
    "        imgData: image data to pad\n",
    "        pad: amount of padding per layer\n",
    "\n",
    "        Results:\n",
    "        return : image with desired padding\n",
    "        '''\n",
    "        X = np.pad(imgData,((0,0),(pad,pad),(pad,pad),(0,0)),'constant',constant_values = 0)\n",
    "        return X\n",
    "\n",
    "    def conv2d(self,\n",
    "        in_planes: int,\n",
    "        out_planes: int,\n",
    "        kernel_size: int,\n",
    "        activation: str,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0) -> None:\n",
    "        '''\n",
    "        Add paramters for this layer in the parameters list; initialize the weights for the network\n",
    "        \n",
    "        Parameters:\n",
    "        in_planes:\n",
    "        out_planes:\n",
    "        kernel_size: \n",
    "        activation:\n",
    "        stride:\n",
    "        padding:\n",
    "        \n",
    "        Results:\n",
    "        return : None\n",
    "        '''\n",
    "        num_layers = int(len(self.parameters)/2)\n",
    "        self.parameters['W'+str(num_layers+1)] = np.random.randn(kernel_size,kernel_size,in_planes,out_planes)\n",
    "        self.parameters['b'+str(num_layers+1)] = np.random.randn(1,1,1,out_planes)\n",
    "        self.activations.append(activation)\n",
    "        self.layer_type.append('conv')\n",
    "        self.hyperparam[num_layers+1] = list((stride,padding))\n",
    "\n",
    "    def conv_single(self,\n",
    "        a_prev_slice: np.array,\n",
    "        W: np.array,\n",
    "        b: np.array) -> float:\n",
    "        '''\n",
    "        Apply convolution using W and b as filter on the activation slice of the previous layer\n",
    "\n",
    "        Parameters:\n",
    "        a_prev_slice: a slice of previous activated layer\n",
    "        W           : Filter\n",
    "        b           : bais\n",
    "        \n",
    "        Results:\n",
    "        return Z: scalar value resultant of the convolution\n",
    "        '''\n",
    "        Z  = np.multiply(a_prev_slice,W)\n",
    "        Z = np.sum(Z)\n",
    "        Z = Z + float(b) #to convert the value to float from matrix type\n",
    "        return Z\n",
    "    \n",
    "    def conv_forward(self,\n",
    "        A_prev: np.array,\n",
    "        W: np.array,\n",
    "        b: np.array,\n",
    "        hyper_param: list) -> Tuple[np.array, Tuple[np.array, np.array, np.array, list]]:\n",
    "        '''\n",
    "        Implements forward pass of convolutional layer.\n",
    "        \n",
    "        Parameters:\n",
    "        A_prev:activations of previous layer\n",
    "        W: Filter\n",
    "        b: bias\n",
    "        hyper_param  : list of hyperparameters, stride and padding\n",
    "\n",
    "        Results:\n",
    "        return: Z, cache\n",
    "        '''\n",
    "        m,h_prev,w_prev,nc_prev = A_prev.shape\n",
    "        f,f,nc_prev,nc = W.shape\n",
    "        stride,pad = hyper_param\n",
    "        #comupte the dimensions of the result using convolution formula => w/h = (w/h(prev) -f +2*pad)/stride +1\n",
    "        n_h = int(np.floor((h_prev-f+2*pad)/stride)) +1\n",
    "        n_w = int(np.floor((w_prev-f+2*pad)/stride)) +1\n",
    "        \n",
    "        Z = np.zeros((m,n_h,n_w,nc))\n",
    "        A_prev_pad = self.zero_pad(A_prev,pad)\n",
    "        for i in range(m):\n",
    "            prev_pad = A_prev_pad[i]\n",
    "\n",
    "            for h in range(n_h):\n",
    "                for w in range(n_w):\n",
    "                    for c in range(nc):\n",
    "                        vertstart = h*stride\n",
    "                        vertend = vertstart + f\n",
    "                        horstart = w*stride\n",
    "                        horend = horstart+f \n",
    "\n",
    "                        prev_slice = prev_pad[vertstart:vertend,horstart:horend,:]\n",
    "                        Z[i,h,w,c] = self.conv_single(prev_slice,W[:,:,:,c],b[:,:,:,c])\n",
    "        \n",
    "        cache = (A_prev,W,b,hyper_param)\n",
    "\n",
    "        return Z,cache\n",
    "\n",
    "    def pool_forward(self,\n",
    "        A_prev: np.array,\n",
    "        f: int,\n",
    "        stride: int,\n",
    "        type='max'):\n",
    "        '''\n",
    "        To enable max and average pooling during the forward pass\n",
    "        \n",
    "        Parameters:\n",
    "        A_prev: Activation of previous layer\n",
    "        f: filter size\n",
    "        stride: size of each stride\n",
    "        type: type of pooling, max or average\n",
    "        \n",
    "        Results:\n",
    "        returns A,cache:\n",
    "        '''\n",
    "        #Calculate the resultant dimensions:\n",
    "        n_h = int(1 + (A_prev.shape[1] - f) / stride)\n",
    "        n_w = int(1 + (A_prev.shape[2] - f) / stride)\n",
    "        n_c = A_prev.shape[3]\n",
    "\n",
    "        A = np.zeros((A_prev.shape[0],n_h,n_w,n_c))\n",
    "\n",
    "        for i in range(A.shape[0]):\n",
    "            for h in range(n_h):\n",
    "                for w in range(n_w):\n",
    "                    for c in range(n_c):\n",
    "                        vertstart = h*stride\n",
    "                        vertend = vertstart + f\n",
    "                        horstart = w*stride\n",
    "                        horend = horstart+f \n",
    "\n",
    "                        a_prev_slice = A_prev[i,vertstart:vertend,horstart:horend,c]\n",
    "\n",
    "                        if type == 'max':\n",
    "                            A[i,h,w,c] = np.max(a_prev_slice)\n",
    "                        elif type == 'avg':\n",
    "                            A[i,h,w,c] = np.mean(a_prev_slice)\n",
    "        \n",
    "        cache = (A_prev,[f,stride],type)\n",
    "        return A,cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def conv_backward(self,\n",
    "        dZ: ndarray, \n",
    "        cache: Tuple[np.array, np.array, np.array, List[int]]) -> Tuple[np.array, np.array, np.array]:\n",
    "        '''\n",
    "        Implement the backward propagation for a convolution function\n",
    "        \n",
    "        Parameters:\n",
    "        dZ: adient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "        cache: cache of values needed for the conv_backward(), output of conv_forward()\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev: gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                   numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        dW: gradient of the cost with respect to the weights of the conv layer (W)\n",
    "              numpy array of shape (f, f, n_C_prev, n_C)\n",
    "        db: gradient of the cost with respect to the biases of the conv layer (b)\n",
    "              numpy array of shape (1, 1, 1, n_C)\n",
    "        '''\n",
    "        \n",
    "\n",
    "        (A_prev, W, b, hparameters) = cache\n",
    "        \n",
    "        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "        \n",
    "        (f, f, n_C_prev, n_C) = W.shape\n",
    "        \n",
    "        stride = hparameters[0]\n",
    "        pad = hparameters[1]\n",
    "        \n",
    "        (m, n_H, n_W, n_C) = dZ.shape\n",
    "        \n",
    "        dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "        dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "        db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "        A_prev_pad = self.zero_pad(A_prev, pad)\n",
    "        dA_prev_pad = self.zero_pad(dA_prev, pad)\n",
    "        \n",
    "        for i in range(m):                      \n",
    "            \n",
    "            a_prev_pad = A_prev_pad[i]\n",
    "            da_prev_pad = dA_prev_pad[i]\n",
    "            \n",
    "            for h in range(n_H):                  \n",
    "                for w in range(n_W):               \n",
    "                    for c in range(n_C):           \n",
    "                        \n",
    "                        vert_start = h * stride\n",
    "\n",
    "                        vert_end = vert_start + f\n",
    "                        horiz_start = w * stride\n",
    "\n",
    "                        horiz_end = horiz_start + f\n",
    "                        \n",
    "                        a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                        da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                        dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                        db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                        \n",
    "            dA_prev[i, :, :, :] =  da_prev_pad if pad == 0 else da_prev_pad[pad:-pad,pad:-pad,:]\n",
    "        \n",
    "        assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "        \n",
    "        return dA_prev, dW, db\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "    def create_mask(self,X):\n",
    "        '''\n",
    "        Creates mask of from a slice which sets max element index to 1 and others to 0\n",
    "\n",
    "        :param X: original matrix\n",
    "        :return :mask\n",
    "        '''\n",
    "        mask = (X==np.max(X))\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def average_back(self,X,shape):\n",
    "        '''\n",
    "        Computes backward pass for average pooling layer\n",
    "\n",
    "        :param X: average pooled layer\n",
    "        :param shape: shape of the original matrix\n",
    "        '''\n",
    "        h,w = shape\n",
    "        X = X/(h*w)\n",
    "        return np.ones(shape)*X\n",
    "\n",
    "    def pool_backward(self,\n",
    "        dA, \n",
    "        cache, \n",
    "        mode = \"max\"):\n",
    "        \"\"\"\n",
    "        Implements the backward pass of the pooling layer\n",
    "        \n",
    "        :param dA: gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "        :param cache: cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "        :param mode:the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev  gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "        \"\"\"\n",
    "        \n",
    "        (A_prev, (stride,f),type) = cache\n",
    "        \n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "        m, n_H, n_W, n_C = dA.shape\n",
    "        \n",
    "        dA_prev = np.zeros(A_prev.shape)\n",
    "        \n",
    "        for i in range(m):                       \n",
    "            a_prev = A_prev[i]\n",
    "            for h in range(n_H):                   \n",
    "                for w in range(n_W):               \n",
    "                    for c in range(n_C):           \n",
    "                        \n",
    "                        vert_start = h*stride\n",
    "                        vert_end = vert_start + f\n",
    "                        horiz_start = w*stride\n",
    "                        horiz_end = horiz_start + f\n",
    "                        \n",
    "                        \n",
    "                        if type == \"max\":\n",
    "                        \n",
    "                            a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                            mask = self.create_mask(a_prev_slice)\n",
    "                            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        elif mode == \"average\":\n",
    "                            da = dA[i, h, w, c]\n",
    "                            shape = (f, f)\n",
    "                            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += self.average_back(da, shape)\n",
    "                        \n",
    "    \n",
    "    \n",
    "        return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
