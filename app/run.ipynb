{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c268fa2-808d-436b-ad5e-7cbedcc8e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from neural_networks.nn import NeuralNetwork\n",
    "from neural_networks.optimizer import Optimizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4031b65-87ce-46e9-b440-4b8b5c5b79ba",
   "metadata": {},
   "source": [
    "## 1. Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabeb137-740e-41d8-b8ed-dc24706ae12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a binary classification test\n"
     ]
    }
   ],
   "source": [
    "# test run for binary classification problem:\n",
    "np.random.seed(3)\n",
    "print('Running a binary classification test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c16b81-6fd5-4573-aa7e-dbd9b368be8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (10, 30000)\n",
      "Output shape:  (1, 30000)\n"
     ]
    }
   ],
   "source": [
    "data = datasets.make_classification(n_samples=30000,n_features=10,n_classes=2)\n",
    "X = data[0].T\n",
    "Y = (data[1].reshape(30000,1)).T\n",
    "\n",
    "print(\"Input shape: \", X.shape)\n",
    "print(\"Output shape: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaeb286-cff9-46f8-979f-96aa83200be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Network Architecture ====================\n",
      "----->> Layer 1: ( 10,20 ) |  activation: relu | No of parameters: 200\n",
      "----->> Layer 2: ( 20,1 ) |  activation: sigmoid | No of parameters: 20\n",
      "\n",
      "==================== Optimizer: gradient-descent ====================\n",
      "----->> Loss at  0 :  0.7462\n",
      "----->> Loss at  100 :  0.24641\n",
      "\n",
      "for gradient descent \n",
      " accuracy =  91.65\n"
     ]
    }
   ],
   "source": [
    "#Generate sample binary classification data\n",
    "net = NeuralNetwork(\n",
    "                    layer_dimensions=[10,20,1],\n",
    "                    activations=['relu','sigmoid']\n",
    "                   )\n",
    "net.cost_function = 'CrossEntropyLoss'\n",
    "print(net)\n",
    "\n",
    "#Optimize using standard gradient descenet\n",
    "optim = Optimizer.gradientDescentOptimizer\n",
    "optim(input=X,\n",
    "      mappings=Y,\n",
    "      net=net,\n",
    "      alpha=0.07,\n",
    "      epoch=200,\n",
    "      lamb=0.05,\n",
    "      print_at=100)\n",
    "output = net.forward(X)\n",
    "\n",
    "#Convert the probabilities to output values\n",
    "output = 1*(output>=0.5)\n",
    "accuracy = np.sum(output==Y)/30000\n",
    "print()\n",
    "print('for gradient descent \\n accuracy = ' , np.round(accuracy*100,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dcbee3-4d95-4507-a777-6eb7b09aca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Network Architecture ====================\n",
      "----->> Layer 1: ( 10,20 ) |  activation: relu | No of parameters: 200\n",
      "----->> Layer 2: ( 20,30 ) |  activation: relu | No of parameters: 600\n",
      "----->> Layer 3: ( 30,20 ) |  activation: relu | No of parameters: 600\n",
      "----->> Layer 4: ( 20,1 ) |  activation: sigmoid | No of parameters: 20\n",
      "\n",
      "==================== Optimizer: gradient-descent ====================\n",
      "----->> Loss at  0 :  0.74283\n",
      "----->> Loss at  100 :  0.24941\n",
      "\n",
      "for gradient descent \n",
      " accuracy =  92.05333\n"
     ]
    }
   ],
   "source": [
    "#Generate sample binary classification data\n",
    "net = NeuralNetwork(layer_dimensions=[10, 20, 30, 20,1],\n",
    "                    activations=['relu', 'relu', 'relu','sigmoid'])\n",
    "net.cost_function = 'CrossEntropyLoss'\n",
    "print(net)\n",
    "\n",
    "#Optimize using standard gradient descenet\n",
    "optim = Optimizer.gradientDescentOptimizer\n",
    "optim(input=X,\n",
    "      mappings=Y,\n",
    "      net=net,\n",
    "      alpha=0.07,\n",
    "      epoch=200,\n",
    "      lamb=0.05,\n",
    "      print_at=100)\n",
    "output = net.forward(X)\n",
    "\n",
    "#Convert the probabilities to output values\n",
    "output = 1*(output>=0.5)\n",
    "accuracy = np.sum(output==Y)/30000\n",
    "print()\n",
    "print('for gradient descent \\n accuracy = ' , np.round(accuracy*100,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186289b-3715-42fd-a017-4018291fff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Network Architecture ====================\n",
      "----->> Layer 1: ( 10,20 ) |  activation: relu | No of parameters: 200\n",
      "----->> Layer 2: ( 20,1 ) |  activation: sigmoid | No of parameters: 20\n",
      "\n",
      "==================== Optimizer: stochastic-gradient-descent ====================\n",
      "----->> Loss at  1 :  0.21804\n",
      "----->> Loss at  2 :  0.20362\n",
      "----->> Loss at  3 :  0.19824\n",
      "----->> Loss at  4 :  0.19567\n",
      "----->> Loss at  5 :  0.19408\n",
      "\n",
      "for stochastic gradient descent without momentum\n",
      " accuracy =  92.84\n"
     ]
    }
   ],
   "source": [
    "#Optimize using SGD without momentum\n",
    "net = NeuralNetwork(layer_dimensions=[10,20,1],\n",
    "                    activations=['relu','sigmoid'])\n",
    "net.cost_function = 'CrossEntropyLoss'\n",
    "print(net)\n",
    "\n",
    "optim = Optimizer.SGDOptimizer\n",
    "optim(input=X,\n",
    "      mappings=Y,\n",
    "      net=net,\n",
    "      mini_batch_size=128,\n",
    "      alpha=0.07,\n",
    "      epoch=5,\n",
    "      lamb=0.05,\n",
    "      print_at=1)\n",
    "output = net.forward(X)\n",
    "output = 1*(output>=0.5)\n",
    "accuracy = np.sum(output==Y)/30000\n",
    "print()\n",
    "print('for stochastic gradient descent without momentum\\n accuracy = ' , np.round(accuracy*100,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af4920-ed3f-4117-8a01-c3ae699bfd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Network Architecture ====================\n",
      "----->> Layer 1: ( 10,20 ) |  activation: relu | No of parameters: 200\n",
      "----->> Layer 2: ( 20,1 ) |  activation: sigmoid | No of parameters: 20\n",
      "\n",
      "==================== Optimizer: stochastic-gradient-descent-with-momentum ====================\n",
      "----->> Loss at  1 :  0.21864\n",
      "----->> Loss at  2 :  0.20707\n",
      "----->> Loss at  3 :  0.201\n",
      "----->> Loss at  4 :  0.19698\n",
      "----->> Loss at  5 :  0.19414\n",
      "for stochastic gradient descent with momentum\n",
      " accuracy =  92.92\n"
     ]
    }
   ],
   "source": [
    "#optimize using  SGD with momentum\n",
    "net = NeuralNetwork(layer_dimensions=[10,20,1],\n",
    "                    activations=['relu','sigmoid'])\n",
    "net.cost_function = 'CrossEntropyLoss'\n",
    "print(net)\n",
    "\n",
    "optim = Optimizer.SGDOptimizer\n",
    "optim(input=X,\n",
    "      mappings=Y,\n",
    "      net=net,\n",
    "      mini_batch_size=128,\n",
    "      alpha=0.07,\n",
    "      epoch=5,\n",
    "      lamb=0.05,\n",
    "      print_at=1,\n",
    "      momentum=0.9)\n",
    "output = net.forward(X)\n",
    "output = 1*(output>=0.5)\n",
    "accuracy = np.sum(output==Y)/30000\n",
    "print('for stochastic gradient descent with momentum\\n accuracy = ' ,np.round(accuracy*100,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5a3e8-02f7-441e-a76c-67cda844f551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Network Architecture ====================\n",
      "----->> Layer 1: ( 10,20 ) |  activation: relu | No of parameters: 200\n",
      "----->> Layer 2: ( 20,1 ) |  activation: sigmoid | No of parameters: 20\n",
      "\n",
      "==================== Optimizer: adaptive-momentum-estimation(ADAM) ====================\n",
      "----->> Loss at  5 :  0.242\n",
      "----->> Loss at  10 :  0.254\n",
      "----->> Loss at  15 :  0.21214\n",
      "----->> Loss at  20 :  0.19643\n",
      "----->> Loss at  25 :  0.18708\n",
      "----->> Loss at  30 :  0.17332\n",
      "----->> Loss at  35 :  0.16428\n",
      "----->> Loss at  40 :  0.15452\n",
      "----->> Loss at  45 :  0.14755\n",
      "----->> Loss at  50 :  0.14249\n",
      "----->> Loss at  55 :  0.13837\n",
      "----->> Loss at  60 :  0.135\n",
      "----->> Loss at  65 :  0.13149\n",
      "----->> Loss at  70 :  0.12747\n",
      "----->> Loss at  75 :  0.12243\n",
      "----->> Loss at  80 :  0.11836\n",
      "for Adam: \n",
      " accuracy =  95.97333333333333\n"
     ]
    }
   ],
   "source": [
    "#optimize using  ADAM\n",
    "net = NeuralNetwork(layer_dimensions=[10,20,1],\n",
    "                    activations=['relu','sigmoid'])\n",
    "net.cost_function = 'CrossEntropyLoss'\n",
    "print(net)\n",
    "\n",
    "optim = Optimizer.AdamOptimizer\n",
    "optim(input=X,\n",
    "      mappings=Y,\n",
    "      net=net,\n",
    "      alpha=0.07,\n",
    "      epoch=80,\n",
    "      lamb=0.05,\n",
    "      print_at=5)\n",
    "output = net.forward(X)\n",
    "output = 1*(output>=0.5)\n",
    "accuracy = np.sum(output==Y)/30000\n",
    "print('for Adam: \\n accuracy = ' ,accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091868f7-4461-4be7-b414-db9b23ca76c8",
   "metadata": {},
   "source": [
    "## 2. Regression Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571cc6d-0e34-42a3-8d8e-cbbc8bf7bef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a regression test\n"
     ]
    }
   ],
   "source": [
    "#test run for regresssion problem:\n",
    "#Generate sample regression data\n",
    "print('Running a regression test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095a98d-a4b7-430c-8943-17c60626022d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (10, 442)\n",
      "Output shape:  (1, 442)\n"
     ]
    }
   ],
   "source": [
    "X, Y = datasets.load_diabetes(return_X_y=True)\n",
    "X = X.T\n",
    "Y = Y.reshape(442,1).T\n",
    "\n",
    "print(\"Input shape: \", X.shape)\n",
    "print(\"Output shape: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27310d43-5ee5-43f6-80dc-b3b6bb5ecb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Network Architecture ====================\n",
      "----->> Layer 1: ( 10,20 ) |  activation: relu | No of parameters: 200\n",
      "\n",
      "==================== Optimizer: gradient-descent ====================\n",
      "----->> Loss at  0 :  14537.34687\n",
      "----->> Loss at  100 :  10146380.72988\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork(layer_dimensions=[10,20,1],\n",
    "                    activations=['relu'])\n",
    "net.cost_function = 'MSELoss'\n",
    "print(net)\n",
    "\n",
    "#Optimize using standard gradient descenet\n",
    "optim = Optimizer.gradientDescentOptimizer\n",
    "optim(input=X,\n",
    "      mappings=Y,\n",
    "      net=net,\n",
    "      alpha=0.3,\n",
    "      epoch=200,\n",
    "      lamb=0.05,\n",
    "      print_at=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a200ac-9847-4a08-89e5-209354019933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Network Architecture ====================\n",
      "----->> Layer 1: ( 10,20 ) |  activation: relu | No of parameters: 200\n",
      "\n",
      "==================== Optimizer: gradient-descent ====================\n",
      "----->> Loss at  0 :  14529.54303\n",
      "----->> Loss at  100 :  2441.23496\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork(layer_dimensions=[10,20,1],\n",
    "                    activations=['relu'])\n",
    "net.cost_function = 'MSELoss'\n",
    "print(net)\n",
    "\n",
    "#Optimize using standard gradient descent\n",
    "optim = Optimizer.gradientDescentOptimizer\n",
    "optim(input=X,\n",
    "      mappings=Y,\n",
    "      net=net,\n",
    "      alpha=0.03,\n",
    "      epoch=200,\n",
    "      lamb=0.05,\n",
    "      print_at=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc5481-0867-4eb5-bee7-82ceb69cb524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Network Architecture ====================\n",
      "----->> Layer 1: ( 10,30 ) |  activation: relu | No of parameters: 300\n",
      "\n",
      "==================== Optimizer: stochastic-gradient-descent ====================\n",
      "----->> Loss at  1 :  13555.98057\n",
      "----->> Loss at  2 :  9884.72489\n",
      "----->> Loss at  3 :  7487.99573\n",
      "----->> Loss at  4 :  5923.32372\n",
      "----->> Loss at  5 :  4901.84881\n",
      "----->> Loss at  6 :  4234.99267\n",
      "----->> Loss at  7 :  3799.64452\n",
      "----->> Loss at  8 :  3515.43319\n",
      "----->> Loss at  9 :  3329.88946\n",
      "----->> Loss at  10 :  3208.75952\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork(layer_dimensions=[10, 30, 1],\n",
    "                    activations=['relu'])\n",
    "net.cost_function = 'MSELoss'\n",
    "print(net)\n",
    "\n",
    "#Optimize using stochastic gradient descent without momentum\n",
    "optim = Optimizer.SGDOptimizer\n",
    "optim(input=X,\n",
    "      mappings=Y,\n",
    "      net=net,\n",
    "      alpha=0.03,\n",
    "      epoch=10,\n",
    "      lamb=0.05,\n",
    "      print_at=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7acec7-b06e-4dad-86e8-9dce3c5e3bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Network Architecture ====================\n",
      "----->> Layer 1: ( 10,30 ) |  activation: relu | No of parameters: 300\n",
      "\n",
      "for stochastic gradient descent with momentum \n",
      "==================== Optimizer: stochastic-gradient-descent-with-momentum ====================\n",
      "----->> Loss at  1 :  9230.45485\n",
      "----->> Loss at  2 :  4110.65305\n",
      "----->> Loss at  3 :  3338.37177\n",
      "----->> Loss at  4 :  2833.9086\n",
      "----->> Loss at  5 :  2248.05043\n",
      "----->> Loss at  6 :  1675.25794\n",
      "----->> Loss at  7 :  1739.8532\n",
      "----->> Loss at  8 :  1578.87335\n",
      "----->> Loss at  9 :  1543.89075\n",
      "----->> Loss at  10 :  1498.79838\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork([10,30,1],['relu'])\n",
    "net.cost_function = 'MSELoss'\n",
    "print(net)\n",
    "\n",
    "#Optimize using stochaistic gradient descenet with momentum\n",
    "print('for stochastic gradient descent with momentum ')\n",
    "optim = Optimizer.SGDOptimizer\n",
    "optim(input=X,\n",
    "      mappings=Y,\n",
    "      net=net,\n",
    "      alpha=0.03,\n",
    "      epoch=10,\n",
    "      lamb=0.05,\n",
    "      print_at=1,\n",
    "      momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74edbc-2e15-4714-82ac-188cc489c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Network Architecture ====================\n",
      "----->> Layer 1: ( 10,30 ) |  activation: relu | No of parameters: 300\n",
      "\n",
      "for Adam \n",
      "==================== Optimizer: adaptive-momentum-estimation(ADAM) ====================\n",
      "----->> Loss at  5 :  14318.4903\n",
      "----->> Loss at  10 :  13684.97827\n",
      "----->> Loss at  15 :  12565.66751\n",
      "----->> Loss at  20 :  11010.43808\n",
      "----->> Loss at  25 :  9157.68001\n",
      "----->> Loss at  30 :  7200.27325\n",
      "----->> Loss at  35 :  5352.10409\n",
      "----->> Loss at  40 :  3812.56814\n",
      "----->> Loss at  45 :  2722.01095\n",
      "----->> Loss at  50 :  2115.48219\n",
      "----->> Loss at  55 :  1897.78126\n",
      "----->> Loss at  60 :  1879.31865\n",
      "----->> Loss at  65 :  1885.2693\n",
      "----->> Loss at  70 :  1846.15761\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork([10,30,1],['relu'])\n",
    "net.cost_function = 'MSELoss'\n",
    "print(net)\n",
    "\n",
    "#Optimize using stochaistic gradient descenet with momentum\n",
    "print('for Adam ')\n",
    "optim = Optimizer.AdamOptimizer\n",
    "optim(input=X,\n",
    "      mappings=Y,\n",
    "      net=net,\n",
    "      alpha=0.03,\n",
    "      epoch=70,\n",
    "      lamb=0.05,\n",
    "      print_at=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c7ec8-acd6-4ca5-a44c-334ab04396d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Sampe test for COnvolution layers => (NOTE: this is just test for bugs)\n",
    "# net = nn.nn()\n",
    "# net.conv2d(3,5,3,'relu',padding=1)\n",
    "# net.add_fcn([36*5,10,5],['relu','relu'])\n",
    "\n",
    "# X = np.random.randn(10*6*6*3).reshape((10,6,6,3))\n",
    "# out = net.forward(X)\n",
    "# net.cost_function = 'CrossEntropyLoss'\n",
    "# net.backward(out,out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
